    # âœ… FIX 1: Copy vocab to expected path for prepare_csv_wavs.py
    # Script expects vocab at: data/your_training_dataset/vocab.txt
    expected_vocab_dir = "/content/F5-TTS-Vietnamese/data/your_training_dataset"
    os.makedirs(expected_vocab_dir, exist_ok=True)
    expected_vocab_path = f"{expected_vocab_dir}/vocab.txt"
    shutil.copy(str(new_vocab_path), expected_vocab_path)
    print(f"   âœ… Vocab also copied to: {expected_vocab_path}")

# ------------------------------------------------------------------------------
# 3. Run Feature Extraction
# ------------------------------------------------------------------------------
print("\n" + "="*70)
print("ğŸ¨ Running feature extraction...")
print("="*70)

print("â³ This may take 5-10 minutes depending on data size...")

for speaker in speakers:
    print(f"\nğŸ‘¤ Processing {speaker}...")
    
    training_dir = f"/content/data/{speaker}_training"
    
    # Run prepare_csv_wavs.py
    cmd = [
        venv_python,
        "src/f5_tts/train/datasets/prepare_csv_wavs.py",
        training_dir,  # input
        training_dir,  # output
        "--workers", "4"
    ]
    
    print(f"   Running: prepare_csv_wavs.py")
    print(f"   Input/Output: {training_dir}")
    
    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        cwd="/content/F5-TTS-Vietnamese"
    )
    
    if result.returncode != 0:
        print(f"âŒ Feature extraction failed for {speaker}!")
        print(result.stderr)
        # âœ… FIX 3: Mark as not ready if extraction failed
        config['ready_for_training'] = False
        print(f"   âš ï¸  Training will not be ready until feature extraction succeeds")
        continue
    
    # Print output
    print(result.stdout)
    
    # Verify outputs
    required_files = [
        f"{training_dir}/raw.arrow",
        f"{training_dir}/duration.json",
        f"{training_dir}/vocab.txt"
    ]
    
    all_exist = all(os.path.exists(f) for f in required_files)
    
    if all_exist:
        print(f"   âœ… Feature extraction complete!")
        
        # Show file sizes
        arrow_size = os.path.getsize(f"{training_dir}/raw.arrow") / (1024**2)
        print(f"   raw.arrow: {arrow_size:.1f} MB")
    else:
        print(f"   âŒ Some output files missing!")
        for f in required_files:
            exists = "âœ…" if os.path.exists(f) else "âŒ"
            print(f"      {exists} {Path(f).name}")

# ------------------------------------------------------------------------------
# 4. Download Pretrained Model
# ------------------------------------------------------------------------------
print("\n" + "="*70)
print("ğŸ“¥ Downloading pretrained model...")
print("="*70)

for speaker in speakers:
    print(f"\nğŸ‘¤ Setting up for {speaker}...")
    
    # Create checkpoint directory
    ckpt_dir = f"/content/ckpts/{speaker}_training"
    os.makedirs(ckpt_dir, exist_ok=True)
    
    # Download F5-TTS Base model if not exists
    pretrained_path = f"{ckpt_dir}/pretrained_model_1200000.pt"
    
    if not os.path.exists(pretrained_path):
        print("   ğŸ“¥ Downloading F5-TTS Base model (~800MB)...")
        print("   â³ This may take 5-10 minutes...")
        
        # Use cached_path to download
        download_script = f'''
import os
from cached_path import cached_path

url = "hf://SWivid/F5-TTS/F5TTS_Base/model_1200000.pt"
ckpt_path = str(cached_path(url))
print(f"Downloaded to: {{ckpt_path}}")

# Copy to our checkpoint directory
import shutil
shutil.copy(ckpt_path, "{pretrained_path}")
print("âœ… Model ready!")
'''
        
        result = subprocess.run(
            [venv_python, "-c", download_script],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print(result.stdout)
        else:
            print("âŒ Download failed!")
            print(result.stderr)
    else:
        print(f"   âœ… Pretrained model already exists")
    
    # Check if we need to extend embedding
    training_dir = f"/content/data/{speaker}_training"
    vocab_path = f"{training_dir}/vocab.txt"
    
    with open(vocab_path, 'r', encoding='utf-8') as f:
        new_vocab_size = len(f.readlines())
    
    # Pretrained vocab size
    with open(pretrained_vocab_path, 'r', encoding='utf-8') as f:
        pretrained_vocab_size = len(f.readlines())
    
    num_new_tokens = new_vocab_size - pretrained_vocab_size
    
    if num_new_tokens > 0:
        print(f"   ğŸ“ Need to extend embedding: +{num_new_tokens} tokens")
        # Note: We'll handle this in training script
    else:
        print(f"   âœ… Vocab compatible, no extension needed")

# ------------------------------------------------------------------------------
# 5. Update Configuration & Save to Drive
# ------------------------------------------------------------------------------
print("\n" + "="*70)
print("ğŸ’¾ Saving configuration...")
print("="*70)

# Update config
training_dirs = {}
for speaker in speakers:
    training_dirs[speaker] = f"/content/data/{speaker}_training"

config['training_dirs'] = training_dirs
config['speakers_list'] = list(speakers)

# âœ… FIX 3: Only mark ready if all extractions succeeded
# Check if all required files exist for all speakers
all_ready = True
for speaker in speakers:
    training_dir = f"/content/data/{speaker}_training"
    required_files = [
        f"{training_dir}/raw.arrow",
        f"{training_dir}/duration.json",
        f"{training_dir}/vocab.txt"
    ]
    if not all(os.path.exists(f) for f in required_files):
        all_ready = False
        break

config['ready_for_training'] = all_ready

with open(config_path, 'w') as f:
    json.dump(config, f, indent=2)

# Backup everything to Drive
print("\nğŸ“¤ Backing up to Google Drive...")

drive_base = "/content/drive/MyDrive/F5TTS_Vietnamese"

for speaker in speakers:
    training_dir = f"/content/data/{speaker}_training"
    drive_training = f"{drive_base}/training_data/{speaker}"
    
    os.makedirs(drive_training, exist_ok=True)
    
    # Copy important files
    for file in ['metadata.csv', 'vocab.txt', 'raw.arrow', 'duration.json']:
        src = f"{training_dir}/{file}"
        if os.path.exists(src):
            shutil.copy(src, f"{drive_training}/{file}")
    
    print(f"   âœ… Backed up {speaker} to Drive")

# Save config to Drive
drive_config = f"{drive_base}/processing_config.json"
with open(drive_config, 'w') as f:
    json.dump(config, f, indent=2)

print("âœ… All data backed up to Google Drive")

# ------------------------------------------------------------------------------
# 6. Display Summary
# ------------------------------------------------------------------------------
print("\n" + "="*70)
print("âœ… TRAINING DATA READY!")
print("="*70)

print(f"""
ğŸ“Š Preparation Summary:
   Speakers: {len(speakers)}
   
ğŸ‘¥ Per Speaker:
""")

for speaker in speakers:
    training_dir = f"/content/data/{speaker}_training"
    
    try:
        # Count files
        wav_count = len(list(Path(f"{training_dir}/wavs").glob("*.wav")))
        
        # Get vocab size (with check)
        vocab_size = 0
        vocab_path = f"{training_dir}/vocab.txt"
        if os.path.exists(vocab_path):
            with open(vocab_path, 'r', encoding='utf-8') as f:
                vocab_size = len(f.readlines())
        else:
            print(f"   âš ï¸  vocab.txt not found for {speaker}")
        
        # âœ… FIX 2: Get duration (with check)
        total_duration = 0
        duration_path = f"{training_dir}/duration.json"
        if os.path.exists(duration_path):
            with open(duration_path, 'r', encoding='utf-8') as f:
                duration_data = json.load(f)
                total_duration = sum(duration_data['duration']) / 60  # minutes
        else:
            print(f"   âš ï¸  duration.json not found (feature extraction may have failed)")
