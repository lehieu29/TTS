# 11 - FAQ (Frequently Asked Questions)

## ‚ùì General Questions

### Q: F5-TTS l√† g√¨?

**A:** F5-TTS (Flow Matching Text-to-Speech) l√† m·ªôt architecture ti√™n ti·∫øn cho Text-to-Speech, s·ª≠ d·ª•ng Flow Matching thay v√¨ traditional diffusion. N√≥ cho ph√©p zero-shot voice cloning ch·∫•t l∆∞·ª£ng cao.

### Q: T·∫°i sao ch·ªçn F5-TTS thay v√¨ c√°c TTS models kh√°c?

**A:** 
- ‚úÖ **Zero-shot voice cloning:** Ch·ªâ c·∫ßn 5-10s reference audio
- ‚úÖ **High quality:** Naturalness v√† similarity cao
- ‚úÖ **Fast inference:** Flow matching nhanh h∆°n diffusion
- ‚úÖ **Open source:** MIT license, c√≥ th·ªÉ customize
- ‚úÖ **Vietnamese support:** D·ªÖ d√†ng fine-tune cho ti·∫øng Vi·ªát

### Q: D·ª± √°n n√†y kh√°c g√¨ v·ªõi F5-TTS g·ªëc?

**A:**
- ‚úÖ **Optimized cho ti·∫øng Vi·ªát:** Vocabulary, text processing
- ‚úÖ **Complete pipeline:** T·ª´ raw audio ‚Üí trained model
- ‚úÖ **Vietnamese documentation:** H∆∞·ªõng d·∫´n ti·∫øng Vi·ªát
- ‚úÖ **Expansion roadmap:** Audio preprocessing, multi-speaker UI

---

## üìä Data Questions

### Q: C·∫ßn bao nhi√™u d·ªØ li·ªáu ƒë·ªÉ train model?

**A:**
```yaml
Testing: 10 ph√∫t - 1 gi·ªù
  Purpose: Verify pipeline
  Quality: Basic

Single Voice: 5-10 gi·ªù
  Purpose: Clone gi·ªçng c·ª• th·ªÉ
  Quality: Good

Production: 50-100 gi·ªù
  Purpose: High-quality single voice
  Quality: Excellent

Multi-Speaker: 1000+ gi·ªù
  Purpose: Universal TTS system
  Quality: State-of-the-art
```

### Q: D·ªØ li·ªáu c·∫ßn format g√¨?

**A:**
```yaml
Audio:
  Format: WAV (preferred), MP3, FLAC
  Sample Rate: 24kHz
  Channels: Mono
  Duration: 3-10s per file (optimal)
  
Text:
  Format: UTF-8 .txt files
  Content: Exact transcription
  Diacritics: Full Vietnamese diacritics required
  Punctuation: Include for better prosody
```

### Q: C√≥ th·ªÉ d√πng podcast/YouTube audio kh√¥ng?

**A:** **C√≥**, nh∆∞ng c·∫ßn preprocessing:
1. **Voice separation** (Demucs) ƒë·ªÉ t√°ch gi·ªçng/nh·∫°c n·ªÅn
2. **Voice Activity Detection** ƒë·ªÉ detect speech segments
3. **Transcription** (Whisper) ƒë·ªÉ t·∫°o text
4. **Quality filtering** ƒë·ªÉ lo·∫°i b·ªè bad samples

‚Üí Xem [08-EXPANSION-ROADMAP.md](08-EXPANSION-ROADMAP.md) ƒë·ªÉ implement.

### Q: L√†m sao transcribe audio nhanh?

**A:**
```python
# Option 1: Whisper large-v3 (recommended)
import whisper
model = whisper.load_model("large-v3")
result = model.transcribe(audio_path, language="vi")

# Option 2: FPT.AI ASR (higher accuracy, paid)
# API-based

# Option 3: Manual (highest accuracy, slow)
# S·ª≠ d·ª•ng Transcribe tool + human review
```

---

## üéì Training Questions

### Q: Training m·∫•t bao l√¢u?

**A:**
```yaml
GPU: T4 (Google Colab Free)
  10 ph√∫t data: ~30-60 ph√∫t
  1 gi·ªù data: ~2-4 gi·ªù
  10 gi·ªù data: ~1-2 ng√†y
  
GPU: V100
  10 ph√∫t data: ~15-30 ph√∫t
  1 gi·ªù data: ~1-2 gi·ªù
  10 gi·ªù data: ~12-24 gi·ªù
  
GPU: A100
  10 ph√∫t data: ~10-20 ph√∫t
  1 gi·ªù data: ~40-80 ph√∫t
  10 gi·ªù data: ~8-16 gi·ªù
```

### Q: GPU n√†o t·ªët nh·∫•t?

**A:**
```yaml
Budget:
  T4 (16GB): OK cho testing v√† small datasets
  RTX 3060 (12GB): Good cho home/small projects
  
Recommended:
  RTX 3090 (24GB): Best price/performance
  RTX 4090 (24GB): Fastest consumer GPU
  
Professional:
  V100 (32GB): Cloud standard
  A100 (40GB/80GB): Best for large-scale training
```

### Q: C√≥ th·ªÉ train tr√™n CPU kh√¥ng?

**A:** **Technically c√≥**, nh∆∞ng **KH√îNG khuy·∫øn ngh·ªã**:
- ‚ö†Ô∏è R·∫•t ch·∫≠m (10-100x ch·∫≠m h∆°n GPU)
- ‚ö†Ô∏è Ch·ªâ practical cho dataset < 10 ph√∫t
- ‚úÖ OK cho testing code/debugging

### Q: C√≥ th·ªÉ train tr√™n Google Colab Free kh√¥ng?

**A:** **C√≥**, nh∆∞ng c√≥ limitations:
- ‚úÖ T4 GPU (16GB) - ƒë·ªß ƒë·ªÉ train
- ‚ö†Ô∏è 12-hour runtime limit ‚Üí Ph·∫£i save checkpoints th∆∞·ªùng xuy√™n
- ‚ö†Ô∏è Disk space limited ‚Üí Clean up th∆∞·ªùng xuy√™n
- üí° **Recommendation:** Colab Pro ($10/month) ƒë·ªÉ c√≥ V100/A100 v√† longer runtime

### Q: L√†m sao bi·∫øt model ƒë√£ train t·ªët?

**A:** Check c√°c metrics:
```yaml
Loss:
  - Gi·∫£m ƒë·ªÅu qua epochs
  - Converge v·ªÅ < 0.5
  
Audio Quality:
  - Listen to generated samples
  - So s√°nh v·ªõi gi·ªçng g·ªëc
  - Check prosody, pronunciation
  
Metrics (n·∫øu c√≥ validation set):
  - MOS (Mean Opinion Score): > 4.0
  - WER (Word Error Rate): < 5%
  - Speaker Similarity: > 0.8
```

### Q: Model overfitting, l√†m sao?

**A:**
```yaml
Symptoms:
  - Training loss gi·∫£m nh∆∞ng validation loss tƒÉng
  - Generated audio gi·ªëng training samples qu√°
  
Solutions:
  1. More data
  2. Data augmentation
  3. Early stopping
  4. Reduce epochs
  5. Add regularization
```

---

## üé§ Inference Questions

### Q: Inference m·∫•t bao l√¢u?

**A:**
```yaml
GPU: T4
  10s audio: ~2-3s inference (real-time factor: 0.2-0.3x)
  
GPU: V100
  10s audio: ~1-1.5s inference (RTF: 0.1-0.15x)
  
GPU: A100
  10s audio: ~0.5-1s inference (RTF: 0.05-0.1x)
  
CPU: 8 cores
  10s audio: ~15-20s inference (RTF: 1.5-2.0x)
```

### Q: Reference audio c·∫ßn nh∆∞ th·∫ø n√†o?

**A:**
```yaml
Duration: 5-10 gi√¢y (optimal)
Quality:
  ‚úÖ Clear voice, single speaker
  ‚úÖ Minimal background noise
  ‚úÖ Natural prosody
  ‚úÖ Consistent volume
  
Avoid:
  ‚ùå Multiple speakers
  ‚ùå Background music
  ‚ùå Very short (<3s) or long (>15s)
  ‚ùå Lots of pauses/silence
```

### Q: C√≥ c·∫ßn provide reference text kh√¥ng?

**A:**
```yaml
Recommended: Yes
  - Provide accurate transcription
  - Better quality results
  
Optional: No
  - Model t·ª± ƒë·ªông d√πng Whisper ƒë·ªÉ transcribe
  - May not be 100% accurate
  - OK cho English/Chinese
  - Vietnamese n√™n provide manually
```

### Q: L√†m sao generate text d√†i (>100 t·ª´)?

**A:** Model t·ª± ƒë·ªông chia th√†nh chunks:
```python
# Automatic chunking
long_text = """
ƒê√¢y l√† m·ªôt ƒëo·∫°n text r·∫•t d√†i.
N√≥ s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông chia th√†nh nhi·ªÅu chunks nh·ªè.
M·ªói chunk ƒë∆∞·ª£c generate ri√™ng r·∫Ω.
Sau ƒë√≥ ƒë∆∞·ª£c concatenate l·∫°i v·ªõi cross-fade.
"""

# Model handles automatically
audio = model.infer(ref_audio, ref_text, long_text)
```

### Q: Output c√≥ nhi·ªÅu silence, l√†m sao?

**A:**
```bash
# Option 1: Enable remove_silence
f5-tts_infer-cli --remove_silence ...

# Option 2: Adjust NFE steps
--nfe_step 64  # Higher quality, less silence

# Option 3: Post-process
from f5_tts.infer.utils_infer import remove_silence_for_generated_wav
remove_silence_for_generated_wav("output.wav")
```

---

## üîß Technical Questions

### Q: F5-TTS architecture th·∫ø n√†o?

**A:** 
```
Text ‚Üí Transformer Encoder ‚Üí Text Features
                                    ‚Üì
Reference Audio ‚Üí Speaker Embedding ‚îò
                                    ‚Üì
                        Flow Matching (DiT)
                                    ‚Üì
                            Mel-Spectrogram
                                    ‚Üì
                            Vocoder (Vocos)
                                    ‚Üì
                                  Audio
```

### Q: Kh√°c g√¨ v·ªõi traditional TTS?

**A:**
```yaml
Traditional (Tacotron, FastSpeech):
  - Autoregressive or non-autoregressive
  - Requires extensive training data
  - Limited voice cloning capability
  
F5-TTS (Flow Matching):
  - Non-autoregressive
  - Zero-shot voice cloning
  - Faster inference
  - Better quality with less data
```

### Q: C√≥ th·ªÉ customize model architecture kh√¥ng?

**A:** **C√≥**, c√°c options:
```yaml
Model Size:
  - F5TTS_Small: ~100M params, faster, lower quality
  - F5TTS_Base: ~200M params, balanced (default)
  - F5TTS_Large: Custom, higher quality
  
Architecture:
  - DiT (Diffusion Transformer): Default
  - UNetT (U-Net Transformer): E2-TTS style
  - MMDiT (Multi-Modal DiT): Experimental
```

### Q: C√≥ th·ªÉ train t·ª´ scratch kh√¥ng?

**A:** **C√≥**, nh∆∞ng **kh√¥ng khuy·∫øn ngh·ªã**:
- ‚ö†Ô∏è C·∫ßn > 1000 gi·ªù data
- ‚ö†Ô∏è Training time r·∫•t l√¢u (weeks-months)
- ‚ö†Ô∏è Compute cost cao
- üí° **Better:** Fine-tune t·ª´ pretrained model

### Q: Pretrained model ƒë∆∞·ª£c train tr√™n data g√¨?

**A:**
```yaml
Base Model (SWivid/F5-TTS):
  Languages: Chinese + English
  Dataset: Emilia (multi-lingual)
  Duration: ~1000+ hours
  
Vietnamese Model (hynt/F5-TTS-Vietnamese-100h):
  Language: Vietnamese
  Base: Fine-tuned from SWivid/F5-TTS
  Duration: ~100 hours
```

---

## üí° Best Practices Questions

### Q: Tips ƒë·ªÉ c√≥ model quality t·ªët nh·∫•t?

**A:**
```yaml
Data Quality (most important):
  1. Accurate transcription (99%+)
  2. Clean audio (SNR > 20dB)
  3. Consistent speaker
  4. Natural prosody
  
Data Quantity:
  5. Minimum 10 hours
  6. Recommended 50-100 hours
  7. Diverse content
  
Training:
  8. Fine-tune t·ª´ pretrained
  9. Monitor loss curves
  10. Save best checkpoints
  11. Test regularly
```

### Q: L√†m sao t·ªëi ∆∞u inference speed?

**A:**
```yaml
Hardware:
  1. Use GPU (T4 minimum)
  2. fp16 inference
  3. Batch inference (n·∫øu c√≥ nhi·ªÅu texts)
  
Parameters:
  4. Lower NFE steps (16 thay v√¨ 32)
  5. Cache speaker embeddings
  6. Preload model
  
Code:
  7. Use TorchScript (if applicable)
  8. ONNX export (advanced)
```

### Q: C√≥ th·ªÉ commercial s·ª≠ d·ª•ng kh√¥ng?

**A:**
```yaml
License: MIT (permissive)
  ‚úÖ Can use commercially
  ‚úÖ Can modify
  ‚úÖ Can distribute
  
BUT:
  ‚ö†Ô∏è Voice rights: C·∫ßn consent t·ª´ voice owner
  ‚ö†Ô∏è Model rights: Check pretrained model license
  ‚ö†Ô∏è Data rights: Check dataset licenses
  
Recommendation:
  - Use own recorded data
  - Get explicit permission
  - Consult legal advisor
```

---

## üöÄ Advanced Questions

### Q: Multi-speaker training nh∆∞ th·∫ø n√†o?

**A:**
```python
# Option 1: Single model, speaker embeddings
# Metadata includes speaker_id
speaker_001|wavs/audio_001.wav|xin ch√†o
speaker_002|wavs/audio_002.wav|h√¥m nay tr·ªùi ƒë·∫πp

# Option 2: Separate models per speaker
# Train ri√™ng cho m·ªói speaker
train_speaker("speaker_001", data_001)
train_speaker("speaker_002", data_002)
```

### Q: C√≥ th·ªÉ control emotion kh√¥ng?

**A:**
```yaml
Currently: Limited
  - Model learns prosody t·ª´ training data
  - Reference audio influence emotion
  
Future:
  - Emotion conditioning (planned)
  - Style transfer
  - Prosody control
  
Workaround:
  - Use reference audio v·ªõi desired emotion
  - Fine-tune with emotion-labeled data
```

### Q: L√†m sao implement real-time TTS?

**A:**
```python
# Streaming inference (experimental)
from f5_tts.infer import StreamingTTS

streaming_tts = StreamingTTS(model_path)

# Generate incrementally
for chunk in text_chunks:
    audio_chunk = streaming_tts.generate_chunk(chunk)
    play_audio(audio_chunk)  # Play while generating
```

### Q: C√≥ API server s·∫µn kh√¥ng?

**A:** Ch∆∞a c√≥ official, nh∆∞ng c√≥ th·ªÉ build:
```python
# Example v·ªõi FastAPI
from fastapi import FastAPI, File
from f5_tts.api import F5TTS

app = FastAPI()
tts = F5TTS()

@app.post("/tts")
async def generate_speech(
    text: str,
    speaker: str = "default"
):
    audio = tts.infer(text=text, speaker=speaker)
    return {"audio": audio}

# Run: uvicorn server:app --host 0.0.0.0 --port 8000
```

---

## üåê Deployment Questions

### Q: Deploy l√™n production nh∆∞ th·∫ø n√†o?

**A:**
```yaml
Options:
  1. Docker Container
     - Package model + dependencies
     - Deploy on AWS/GCP/Azure
     
  2. API Service
     - FastAPI / Flask
     - Load balancer
     - GPU instances
     
  3. Edge Deployment
     - ONNX export
     - TensorRT optimization
     - Mobile/embedded devices
     
  4. Serverless
     - AWS Lambda (CPU inference)
     - Google Cloud Functions
     - Azure Functions
```

### Q: L√†m sao scale cho nhi·ªÅu users?

**A:**
```yaml
Architecture:
  Load Balancer
      ‚Üì
  Multiple Inference Servers (GPU)
      ‚Üì
  Model Cache (Redis)
      ‚Üì
  Storage (S3/GCS)
  
Optimization:
  - Batch inference
  - Model caching
  - Request queuing
  - Autoscaling
```

---

## üìö Learning Resources

### Q: H·ªçc th√™m v·ªÅ F5-TTS ·ªü ƒë√¢u?

**A:**
- üìÑ **Paper:** [F5-TTS ArXiv](https://arxiv.org/abs/2410.06885)
- üíª **Code:** [GitHub Repo](https://github.com/SWivid/F5-TTS)
- üéÆ **Demo:** [HuggingFace Space](https://huggingface.co/spaces/hynt/F5-TTS-Vietnamese-100h)
- üìñ **Memory Bank:** Docs trong `memory-bank/`

### Q: C·ªông ƒë·ªìng Vietnamese TTS ·ªü ƒë√¢u?

**A:**
- GitHub Issues: [F5-TTS-Vietnamese](https://github.com/nguyenthienhy/F5-TTS-Vietnamese)
- Discord/Telegram: (TBD)
- Facebook Groups: AI Vietnam communities

---

**Prev:** [`10-TROUBLESHOOTING.md`](10-TROUBLESHOOTING.md)  
**Back to Index:** [`00-INDEX.md`](00-INDEX.md)



