# 04 - Training Pipeline

## ğŸ”„ Complete Training Pipeline

Training pipeline bao gá»“m 6 stages chÃ­nh, Ä‘Æ°á»£c quáº£n lÃ½ bá»Ÿi `fine_tuning.sh`.

---

## ğŸ“Š Pipeline Overview

```
Stage 0: Convert Sample Rate
    â†“
Stage 1: Prepare Metadata
    â†“
Stage 2: Check Vocabulary
    â†“
Stage 3: Extend Embedding
    â†“
Stage 4: Feature Extraction
    â†“
Stage 5: Fine-tuning
    â†“
Trained Model
```

---

## ğŸ¯ Stage 0: Convert Sample Rate

### Purpose
Chuyá»ƒn Ä‘á»•i táº¥t cáº£ audio vá» 24kHz mono (yÃªu cáº§u cá»§a F5-TTS).

### Script
`convert_sr.py`

### Process
```python
# Input: data/your_dataset/*.wav (báº¥t ká»³ sample rate nÃ o)
# Output: data/your_dataset/*.wav (24kHz mono)

for audio_file in dataset:
    sox audio_file -r 24000 -c 1 output_file
```

### Technical Details
```bash
# Tool: sox
# Command: sox input.wav -r 24000 -c 1 output.wav
# Parameters:
#   -r 24000: Resample to 24kHz
#   -c 1: Convert to mono
```

### Why 24kHz?
- F5-TTS model Ä‘Æ°á»£c train vá»›i 24kHz
- Balance giá»¯a quality vÃ  compute
- Standard cho modern TTS

### Skip Condition
Náº¿u audio cá»§a báº¡n Ä‘Ã£ lÃ  24kHz mono, set `stage=1` Ä‘á»ƒ bá» qua.

---

## ğŸ“ Stage 1: Prepare Metadata

### Purpose
Táº¡o file metadata.csv chá»©a mapping audio â†” text vÃ  vocab.

### Script
`prepare_metadata.py`

### Input
```
data/your_dataset/
â”œâ”€â”€ audio_001.wav
â”œâ”€â”€ audio_001.txt  â†’ "xin chÃ o cÃ¡c báº¡n"
â”œâ”€â”€ audio_002.wav
â”œâ”€â”€ audio_002.txt  â†’ "hÃ´m nay trá»i Ä‘áº¹p"
â””â”€â”€ ...
```

### Output
```
data/your_training_dataset/
â”œâ”€â”€ wavs/                    # Copied audio files
â”‚   â”œâ”€â”€ audio_001.wav
â”‚   â”œâ”€â”€ audio_002.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ metadata.csv             # Audio-text pairs
â””â”€â”€ vocab_your_dataset.txt   # Character vocabulary
```

### metadata.csv Format
```csv
wavs/audio_001.wav|xin chÃ o cÃ¡c báº¡n
wavs/audio_002.wav|hÃ´m nay trá»i Ä‘áº¹p
wavs/audio_003.wav|tÃ´i lÃ  trá»£ lÃ½ áº£o
```

### vocab_your_dataset.txt
```txt
 
a
Ã 
Ã¡
áº£
Ã£
áº¡
Äƒ
b
c
...
```

### Filtering Rules
```python
# Loáº¡i bá» audio khÃ´ng há»£p lá»‡
if duration < 1 or duration > 30:
    skip  # Too short or too long
    
if len(text.split()) < 3:
    skip  # Text too short
```

### Code Flow
```python
def process_dataset():
    wav_paths = glob("data/your_dataset/*.wav")
    tokens = set()
    
    with open("metadata.csv", "w") as fw:
        for wav_path in wav_paths:
            # Read text
            txt_path = wav_path.replace(".wav", ".txt")
            text = open(txt_path).read().strip().lower()
            
            # Check duration
            duration = get_audio_duration(wav_path)
            if duration < 1 or duration > 30:
                continue
                
            # Copy audio
            shutil.copy(wav_path, f"wavs/{basename(wav_path)}")
            
            # Write metadata
            fw.write(f"wavs/{basename(wav_path)}|{text}\n")
            
            # Collect vocab
            tokens.update(text)
    
    # Save vocab
    with open("vocab.txt", "w") as fv:
        fv.write("\n".join(sorted(tokens)))
```

---

## ğŸ” Stage 2: Check Vocabulary

### Purpose
TÃ¬m cÃ¡c token trong dataset mÃ  chÆ°a cÃ³ trong pretrained model vocab.

### Script
`check_vocab_pretrained.py`

### Process
```python
# Load vocabs
pretrained_vocab = load("data/Emilia_ZH_EN_pinyin/vocab.txt")
dataset_vocab = load("data/your_training_dataset/vocab_your_dataset.txt")

# Find missing tokens
missing = []
for token in dataset_vocab:
    if token not in pretrained_vocab:
        missing.append(token)

# Create new vocab
new_vocab = pretrained_vocab + missing
save("data/your_training_dataset/vocab.txt", new_vocab)
```

### Why This Matters
- Pretrained model cÃ³ vocab cho Chinese + English
- Tiáº¿ng Viá»‡t cÃ³ cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t: Äƒ, Ã¢, Ä‘, Ãª, Ã´, Æ¡, Æ° vÃ  dáº¥u thanh
- Cáº§n thÃªm tokens nÃ y vÃ o model

### Output
```
Sá»‘ token thiáº¿u trong vocab pretrained: 42
Vocab má»›i Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i data/your_training_dataset/vocab.txt
Tá»•ng sá»‘ token: 812
```

### Common Missing Tokens (Vietnamese)
```txt
Äƒ áº¯ áº± áº³ áºµ áº·
Ã¢ áº¥ áº§ áº© áº« áº­
Ä‘
Ãª áº¿ á» á»ƒ á»… á»‡
Ã´ á»‘ á»“ á»• á»— á»™
Æ¡ á»› á» á»Ÿ á»¡ á»£
Æ° á»© á»« á»­ á»¯ á»±
```

---

## ğŸ”§ Stage 3: Extend Embedding

### Purpose
Má»Ÿ rá»™ng embedding layer cá»§a pretrained model Ä‘á»ƒ support tokens má»›i.

### Script
`extend_embedding_pretrained.py`

### Process
```python
# Load checkpoint
ckpt = torch.load("pretrained_model_1200000.pt")

# Get current embedding
old_embed = ckpt["ema_model.transformer.text_embed.weight"]
vocab_old, embed_dim = old_embed.shape  # e.g., [770, 512]

# Calculate new size
num_new_tokens = 42  # From Stage 2
vocab_new = vocab_old + num_new_tokens  # 770 + 42 = 812

# Create new embedding
new_embed = torch.zeros(vocab_new, embed_dim)
new_embed[:vocab_old] = old_embed  # Copy old weights
new_embed[vocab_old:] = torch.randn(num_new_tokens, embed_dim)  # Initialize new

# Save
ckpt["ema_model.transformer.text_embed.weight"] = new_embed
torch.save(ckpt, "pretrained_model_extended.pt")
```

### Key Points
- **Preserve old weights:** Giá»¯ nguyÃªn embedding cá»§a tokens Ä‘Ã£ há»c
- **Random init new weights:** Khá»Ÿi táº¡o ngáº«u nhiÃªn cho tokens má»›i
- **Seed control:** Set seed=666 Ä‘á»ƒ reproducible

### File Paths
```python
# Input
ckpt_path = "hf://SWivid/F5-TTS/F5TTS_Base/model_1200000.pt"

# Output
new_ckpt_path = "ckpts/your_training_dataset/pretrained_model_1200000.pt"
```

---

## ğŸ¨ Stage 4: Feature Extraction

### Purpose
Preprocess táº¥t cáº£ audio + text thÃ nh features ready for training.

### Script
`src/f5_tts/train/datasets/prepare_csv_wavs.py`

### Process
```python
# Input: metadata.csv + wavs/
# Output: raw.arrow + duration.json + vocab.txt

for audio_path, text in metadata:
    # 1. Load audio
    audio, sr = torchaudio.load(audio_path)
    
    # 2. Get duration
    duration = len(audio) / sr
    
    # 3. Text processing (character tokenization)
    # Note: KhÃ´ng dÃ¹ng pinyin cho tiáº¿ng Viá»‡t
    processed_text = text
    
    # 4. Save to Arrow format
    writer.write({
        "audio_path": audio_path,
        "text": processed_text,
        "duration": duration
    })
```

### Output Files

#### raw.arrow
Binary format chá»©a processed data, nhanh hÆ¡n CSV.

#### duration.json
```json
{
    "duration": [3.2, 5.1, 4.8, 6.3, ...]
}
```

#### vocab.txt (final)
```txt

a
Ã 
Ã¡
...
```

### Performance Optimization
```python
# Multi-threading
MAX_WORKERS = cpu_count() - 1
with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    results = executor.map(process_audio_file, audio_files)

# Chunk processing
CHUNK_SIZE = 100
for chunk in chunks(audio_files, CHUNK_SIZE):
    process_chunk(chunk)
```

### Duration Distribution
```python
# VÃ­ dá»¥ output
For your_training_dataset, sample count: 1247
For your_training_dataset, vocab size is: 87
For your_training_dataset, total 10.52 hours
```

---

## ğŸš€ Stage 5: Fine-tuning

### Purpose
Train model vá»›i dá»¯ liá»‡u cá»§a báº¡n.

### Script
`src/f5_tts/train/finetune_cli.py`

### Command
```bash
python src/f5_tts/train/finetune_cli.py \
    --exp_name "F5TTS_Base" \
    --dataset_name "your_training_dataset" \
    --batch_size_per_gpu 7000 \
    --num_warmup_updates 20000 \
    --save_per_updates 10000 \
    --last_per_updates 10000 \
    --finetune \
    --log_samples \
    --pretrain "ckpts/your_training_dataset/pretrained_model_1200000.pt"
```

### Key Parameters

#### Batch Size
```python
--batch_size_per_gpu 7000  # Frames per batch
# Larger = faster but more memory
# GPU Memory requirements:
#   3200: 8GB
#   7000: 16GB
#   10000: 24GB
```

#### Learning Rate
```python
--learning_rate 1e-5  # Default cho fine-tuning
# KhÃ´ng set quÃ¡ cao â†’ pretrained knowledge bá»‹ destroy
```

#### Warmup
```python
--num_warmup_updates 20000
# Gradually tÄƒng learning rate tá»« 0 â†’ target
# GiÃºp training á»•n Ä‘á»‹nh
```

#### Checkpointing
```python
--save_per_updates 10000     # Save checkpoint má»—i 10k updates
--last_per_updates 10000     # Save model_last.pt má»—i 10k updates
--keep_last_n_checkpoints 3  # Chá»‰ giá»¯ 3 checkpoints gáº§n nháº¥t
```

### Training Loop

```python
for epoch in range(epochs):
    for batch in dataloader:
        # 1. Forward pass
        loss = model(
            inp=batch["audio"],
            text=batch["text"],
            lens=batch["lens"]
        )
        
        # 2. Backward pass
        loss.backward()
        
        # 3. Gradient clipping
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), 
            max_grad_norm=1.0
        )
        
        # 4. Optimizer step
        optimizer.step()
        optimizer.zero_grad()
        
        # 5. EMA update
        ema_model.update()
        
        # 6. Logging
        if step % log_interval == 0:
            logger.log({
                "loss": loss.item(),
                "lr": optimizer.param_groups[0]["lr"],
                "step": step
            })
        
        # 7. Checkpoint saving
        if step % save_per_updates == 0:
            save_checkpoint(
                f"model_{step}.pt",
                model, optimizer, ema_model
            )
```

### Training Monitoring

#### Console Output
```
Epoch 1/100 | Step 100/50000 | Loss: 0.234 | LR: 1.2e-6 | Time: 0.5s/step
Epoch 1/100 | Step 200/50000 | Loss: 0.198 | LR: 2.4e-6 | Time: 0.5s/step
...
```

#### Checkpoint Files
```
ckpts/your_training_dataset/
â”œâ”€â”€ pretrained_model_1200000.pt  # Extended base model
â”œâ”€â”€ model_10000.pt               # Checkpoint at 10k steps
â”œâ”€â”€ model_20000.pt               # Checkpoint at 20k steps
â”œâ”€â”€ model_30000.pt               # Checkpoint at 30k steps
â””â”€â”€ model_last.pt                # Latest checkpoint
```

### Multi-GPU Training

```bash
# Sá»­ dá»¥ng accelerate
accelerate launch src/f5_tts/train/finetune_cli.py \
    --exp_name "F5TTS_Base" \
    --dataset_name "your_training_dataset" \
    --batch_size_per_gpu 7000 \
    ...
```

---

## ğŸ“Š Training Time Estimates

| Dataset Size | Epochs | GPU (T4) | GPU (V100) | CPU |
|--------------|--------|----------|------------|-----|
| 10 phÃºt      | 50     | 30 phÃºt  | 15 phÃºt    | 4h  |
| 1 giá»        | 50     | 2 giá»    | 1 giá»      | 24h |
| 10 giá»       | 50     | 10 giá»   | 5 giá»      | 7d  |
| 100 giá»      | 50     | 3 ngÃ y   | 1.5 ngÃ y   | N/A |

---

## ğŸ¯ Best Practices

### 1. Data Quality > Quantity
```python
# 10 giá» clean audio > 100 giá» noisy audio
- RÃµ rÃ ng, Ã­t noise
- Transcription chÃ­nh xÃ¡c
- Consistent quality
```

### 2. Start Small
```python
# Test vá»›i small dataset trÆ°á»›c
stage = 0
stop_stage = 5
# Cháº¡y full pipeline vá»›i 10 phÃºt data
# Verify everything works
# Sau Ä‘Ã³ scale lÃªn
```

### 3. Monitor Training
```python
# Watch for:
- Loss giáº£m Ä‘á»u
- KhÃ´ng bá»‹ overfitting (náº¿u cÃ³ validation set)
- Audio samples quality (--log_samples)
```

### 4. Checkpoint Management
```python
# LuÃ´n backup:
- pretrained_model_*.pt
- model_last.pt
- Best checkpoint dá»±a trÃªn validation

# XÃ³a intermediate checkpoints náº¿u thiáº¿u disk space
```

---

## ğŸ› Common Issues

### Issue: "CUDA out of memory"
```bash
# Solution: Giáº£m batch_size
--batch_size_per_gpu 3200  # Tá»« 7000 â†’ 3200
```

### Issue: "vocab.txt not found"
```bash
# Solution: Check Stage 2 output
ls data/your_training_dataset/vocab.txt
# Pháº£i tá»“n táº¡i sau Stage 2
```

### Issue: Loss khÃ´ng giáº£m
```python
# Causes:
1. Learning rate quÃ¡ cao â†’ giáº£m xuá»‘ng
2. Data quality kÃ©m â†’ check audio + transcription
3. Batch size quÃ¡ nhá» â†’ tÄƒng lÃªn náº¿u cÃ³ GPU memory
```

---

**Prev:** [`03-ARCHITECTURE.md`](03-ARCHITECTURE.md)  
**Next:** [`05-INFERENCE-PIPELINE.md`](05-INFERENCE-PIPELINE.md)



