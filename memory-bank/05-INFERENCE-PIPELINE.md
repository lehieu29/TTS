# 05 - Inference Pipeline

## üé§ Inference Overview

Inference l√† qu√° tr√¨nh s·ª≠ d·ª•ng model ƒë√£ train ƒë·ªÉ t·∫°o gi·ªçng n√≥i t·ª´ text.

---

## üîÑ Inference Flow

```
Reference Audio + Text
    ‚Üì
Preprocessing
    ‚Üì
Speaker Embedding Extraction
    ‚Üì
Text Encoding
    ‚Üì
Flow Matching Generation
    ‚Üì
Mel-Spectrogram
    ‚Üì
Vocoder (Vocos)
    ‚Üì
Audio Output
```

---

## üõ†Ô∏è Inference Methods

### Method 1: CLI (Command Line)

#### Basic Usage
```bash
f5-tts_infer-cli \
--model "F5TTS_Base" \
--ref_audio ref.wav \
--ref_text "c·∫£ hai b√™n h√£y c·ªë g·∫Øng hi·ªÉu cho nhau" \
--gen_text "xin ch√†o, t√¥i l√† tr·ª£ l√Ω ·∫£o ti·∫øng Vi·ªát" \
--speed 1.0
```

#### With Custom Model
```bash
f5-tts_infer-cli \
--model "F5TTS_Base" \
--ref_audio ref.wav \
--ref_text "xin ch√†o c√°c b·∫°n" \
--gen_text "h√¥m nay tr·ªùi ƒë·∫πp qu√°" \
--speed 1.0 \
--vocoder_name vocos \
--vocab_file data/your_training_dataset/vocab.txt \
--ckpt_file ckpts/your_training_dataset/model_last.pt
```

#### All Parameters
```bash
f5-tts_infer-cli \
--model "F5TTS_Base"              # Model architecture
--ref_audio ref.wav               # Reference audio (gi·ªçng m·∫´u)
--ref_text "text"                 # Text c·ªßa ref_audio (optional)
--gen_text "text to generate"    # Text mu·ªën t·∫°o gi·ªçng
--gen_file output.wav             # Output file path (optional)
--remove_silence                  # Remove silence (optional)
--output_dir "outputs"            # Output directory
--output_format "wav"             # Output format (wav/mp3/flac)
--speed 1.0                       # Speed (0.3-2.0)
--cross_fade_duration 0.15        # Cross-fade (seconds)
--nfe_step 32                     # NFE steps (quality vs speed)
--sway_sampling_coef -1.0         # Sampling coefficient
--cfg_strength 2.0                # CFG strength
--fix_duration None               # Fix duration (seconds)
--vocoder_name vocos              # Vocoder (vocos/bigvgan)
--vocab_file path/to/vocab.txt    # Custom vocab
--ckpt_file path/to/model.pt      # Custom checkpoint
```

### Method 2: Gradio Web UI

#### Launch
```bash
f5-tts_infer-gradio
# Ho·∫∑c
python src/f5_tts/infer/infer_gradio.py
```

#### Access
```
http://localhost:7860
```

#### UI Features

**Tab 1: Basic-TTS**
- Upload reference audio
- Nh·∫≠p reference text (optional - auto-transcribe v·ªõi Whisper)
- Nh·∫≠p text mu·ªën t·∫°o
- Advanced settings:
  - Speed slider (0.3-2.0)
  - NFE steps (4-64)
  - Cross-fade duration
  - Remove silence toggle
- Generate button
- Audio player + Spectrogram visualization

**Tab 2: Multi-Speech**
- Upload multiple speech types/speakers
- Format: `{Speaker1} text here {Speaker2} more text`
- Dynamic speech type addition
- Batch generation

**Tab 3: Voice-Chat**
- AI chat v·ªõi voice output
- Reference audio cho voice
- Microphone input
- Real-time TTS response

### Method 3: Python API

```python
from f5_tts.api import F5TTS

# Initialize
f5tts = F5TTS(
    model_type="F5-TTS",  # or "E2-TTS"
    ckpt_file="path/to/model.pt",
    vocab_file="path/to/vocab.txt"
)

# Generate
audio, sample_rate, spectrogram = f5tts.infer(
    ref_file="ref.wav",
    ref_text="xin ch√†o",
    gen_text="h√¥m nay tr·ªùi ƒë·∫πp",
    speed=1.0
)

# Save
import soundfile as sf
sf.write("output.wav", audio, sample_rate)
```

---

## üéõÔ∏è Key Parameters Explained

### ref_audio (Reference Audio)
**Purpose:** Gi·ªçng m·∫´u ƒë·ªÉ model clone

**Requirements:**
- **Duration:** 3-15 gi√¢y (optimal: 5-10s)
- **Quality:** R√µ r√†ng, √≠t noise
- **Content:** Gi·ªçng n√≥i li√™n t·ª•c, kh√¥ng im l·∫∑ng nhi·ªÅu
- **Format:** WAV, MP3, FLAC

**Tips:**
```python
# Good reference:
‚úÖ Clean speech, single speaker
‚úÖ 5-10 seconds long
‚úÖ Natural prosody
‚úÖ Consistent volume

# Bad reference:
‚ùå Multiple speakers
‚ùå Background music/noise
‚ùå Too short (<3s) or too long (>15s)
‚ùå Lots of pauses
```

### ref_text (Reference Text)
**Purpose:** Text t∆∞∆°ng ·ª©ng v·ªõi ref_audio

**Options:**
1. **Provide manually** (recommended)
   ```bash
   --ref_text "xin ch√†o c√°c b·∫°n"
   ```

2. **Auto-transcribe** (n·∫øu kh√¥ng cung c·∫•p)
   ```bash
   # Kh√¥ng set ref_text
   # ‚Üí Model t·ª± ƒë·ªông d√πng Whisper ƒë·ªÉ transcribe
   # ‚Üí C√≥ th·ªÉ kh√¥ng ch√≠nh x√°c 100%
   ```

**Why it matters:**
- Model c·∫ßn bi·∫øt ref_audio n√≥i g√¨
- Sai ref_text ‚Üí quality gi·∫£m
- Auto-transcribe OK cho ti·∫øng Anh/Trung
- Ti·∫øng Vi·ªát n√™n provide manually

### gen_text (Generation Text)
**Purpose:** Text b·∫°n mu·ªën model n√≥i

**Formatting:**
```python
# Short text
gen_text = "xin ch√†o"

# Long text - t·ª± ƒë·ªông chia chunks
gen_text = """
H√¥m nay tr·ªùi ƒë·∫πp qu√°. 
T√¥i mu·ªën ƒëi ch∆°i. 
B·∫°n c√≥ r·∫£nh kh√¥ng?
"""

# With punctuation
gen_text = "Xin ch√†o! B·∫°n kh·ªèe kh√¥ng?"
```

**Tips:**
- D√πng d·∫•u c√¢u ƒë√∫ng ‚Üí prosody t·ªët h∆°n
- Text d√†i ‚Üí t·ª± ƒë·ªông chia chunks
- Lowercase vs Uppercase: kh√¥ng ·∫£nh h∆∞·ªüng nhi·ªÅu

### speed
**Purpose:** ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô n√≥i

```python
speed = 0.5   # R·∫•t ch·∫≠m
speed = 0.8   # Ch·∫≠m
speed = 1.0   # B√¨nh th∆∞·ªùng (default)
speed = 1.2   # Nhanh
speed = 1.5   # R·∫•t nhanh
speed = 2.0   # Maximum
```

### nfe_step (Number of Function Evaluations)
**Purpose:** S·ªë b∆∞·ªõc sampling trong flow matching

**Trade-off: Quality vs Speed**
```python
nfe_step = 8    # Nhanh nh·∫•t, quality th·∫•p
nfe_step = 16   # Nhanh, quality OK
nfe_step = 32   # Default - balanced
nfe_step = 64   # Ch·∫≠m, quality cao nh·∫•t
```

**Recommendations:**
- Development/testing: 16
- Production: 32
- High quality: 64

### cross_fade_duration
**Purpose:** Th·ªùi gian cross-fade gi·ªØa c√°c chunks

```python
cross_fade_duration = 0.0    # No cross-fade
cross_fade_duration = 0.15   # Default
cross_fade_duration = 0.5    # Smooth transitions
```

**When to use:**
- Text d√†i ƒë∆∞·ª£c chia th√†nh chunks
- Tr√°nh "click" sound gi·ªØa chunks

### remove_silence
**Purpose:** Lo·∫°i b·ªè silence trong output

```python
remove_silence = False  # Default - gi·ªØ nguy√™n
remove_silence = True   # Remove silence
```

**Note:**
- Model c√≥ xu h∆∞·ªõng t·∫°o silence d√†i
- remove_silence gi√∫p output ng·∫Øn g·ªçn h∆°n
- C√≥ th·ªÉ g√¢y artifacts

---

## üîß Advanced Inference

### Long Text Generation

**Problem:** Text d√†i (>100 t·ª´) kh√≥ generate m·ªôt l∆∞·ª£t

**Solution:** Auto-chunking

```python
def chunk_text(text, max_chars=135):
    """
    Chia text th√†nh chunks nh·ªè
    """
    sentences = text.split('. ')
    chunks = []
    buffer = []
    
    for sentence in sentences:
        buffer.append(sentence)
        if len(' '.join(buffer)) > max_chars:
            chunks.append('. '.join(buffer))
            buffer = []
    
    if buffer:
        chunks.append('. '.join(buffer))
    
    return chunks

# Generate t·ª´ng chunk
for chunk in chunk_text(long_text):
    audio_chunk = model.infer(ref_audio, ref_text, chunk)
    audio_segments.append(audio_chunk)

# Concatenate v·ªõi cross-fade
final_audio = concatenate_with_crossfade(
    audio_segments, 
    cross_fade_duration=0.15
)
```

### Multi-Speaker Generation

**Use case:** T·∫°o audio v·ªõi nhi·ªÅu gi·ªçng kh√°c nhau

```python
speakers = {
    "Alice": {
        "ref_audio": "alice_ref.wav",
        "ref_text": "Hello, I'm Alice"
    },
    "Bob": {
        "ref_audio": "bob_ref.wav", 
        "ref_text": "Hi, I'm Bob"
    }
}

script = [
    {"speaker": "Alice", "text": "How are you today?"},
    {"speaker": "Bob", "text": "I'm doing great, thanks!"},
]

audio_segments = []
for line in script:
    speaker_info = speakers[line["speaker"]]
    audio = model.infer(
        ref_file=speaker_info["ref_audio"],
        ref_text=speaker_info["ref_text"],
        gen_text=line["text"]
    )
    audio_segments.append(audio)

# Merge
final_audio = concatenate(audio_segments)
```

### Voice Conversion

**Use case:** Convert gi·ªçng A sang gi·ªçng B

```python
# Source audio
source_audio = "source.wav"
source_text = transcribe(source_audio)  # Whisper

# Target voice
target_ref = "target_ref.wav"
target_ref_text = "sample text"

# Convert
converted_audio = model.infer(
    ref_file=target_ref,
    ref_text=target_ref_text,
    gen_text=source_text
)
```

---

## üìä Performance Optimization

### GPU Inference
```python
# S·ª≠ d·ª•ng float16 cho faster inference
model.to("cuda").half()

# Batch inference (n·∫øu c√≥ nhi·ªÅu texts)
batch_results = model.batch_infer(
    ref_files=[ref_audio] * N,
    ref_texts=[ref_text] * N,
    gen_texts=text_list
)
```

### CPU Inference
```python
# Ch·∫≠m h∆°n nh∆∞ng v·∫´n work
model.to("cpu")

# T·ªëi ∆∞u:
import torch
torch.set_num_threads(8)  # Use multiple CPU cores
```

### Caching
```python
# Cache speaker embedding
speaker_embed = extract_speaker_embedding(ref_audio)

# Reuse cho multiple generations
for text in text_list:
    audio = model.infer_with_embed(
        speaker_embed=speaker_embed,
        gen_text=text
    )
```

---

## üé® Post-Processing

### Normalize Volume
```python
import soundfile as sf
import numpy as np

audio, sr = sf.read("output.wav")

# Normalize to -3dB
audio = audio / np.max(np.abs(audio)) * 0.7

sf.write("output_normalized.wav", audio, sr)
```

### Remove Silence
```python
from f5_tts.infer.utils_infer import remove_silence_for_generated_wav

remove_silence_for_generated_wav("output.wav")
```

### Format Conversion
```python
# WAV to MP3
from pydub import AudioSegment

audio = AudioSegment.from_wav("output.wav")
audio.export("output.mp3", format="mp3", bitrate="192k")
```

---

## üêõ Common Issues

### Issue: Output c√≥ nhi·ªÅu silence
**Solution:**
```bash
--remove_silence
# Ho·∫∑c post-process manually
```

### Issue: Gi·ªçng kh√¥ng gi·ªëng reference
**Causes:**
1. Reference audio quality k√©m
2. Reference audio qu√° ng·∫Øn (<5s)
3. Reference text kh√¥ng ch√≠nh x√°c

**Solutions:**
- D√πng reference 5-10s
- Provide ref_text manually
- Ch·ªçn reference r√µ r√†ng, √≠t noise

### Issue: Output c√≥ artifacts/glitches
**Causes:**
1. NFE steps qu√° th·∫•p
2. Model ch∆∞a train t·ªët
3. Text qu√° d√†i

**Solutions:**
```bash
--nfe_step 64  # TƒÉng quality
# Ho·∫∑c chia text th√†nh chunks nh·ªè h∆°n
```

### Issue: Ti·∫øng Vi·ªát ph√°t √¢m sai
**Causes:**
1. Model ch∆∞a train v·ªõi d·ªØ li·ªáu ti·∫øng Vi·ªát ƒë·ªß
2. Vocab kh√¥ng ƒë·∫ßy ƒë·ªß
3. Text c√≥ k√Ω t·ª± l·∫°

**Solutions:**
- Fine-tune v·ªõi d·ªØ li·ªáu ti·∫øng Vi·ªát
- Check vocab.txt c√≥ ƒë·∫ßy ƒë·ªß k√Ω t·ª± kh√¥ng
- Normalize text (lowercase, remove special chars)

---

## üí° Best Practices

### 1. Reference Audio Selection
```python
‚úÖ DO:
- Ch·ªçn audio r√µ r√†ng, gi·ªçng ƒë∆°n
- Duration: 5-10 gi√¢y
- Natural prosody
- Consistent volume

‚ùå DON'T:
- Nhi·ªÅu speaker
- Background noise/music
- Qu√° ng·∫Øn ho·∫∑c qu√° d√†i
- Im l·∫∑ng nhi·ªÅu
```

### 2. Text Formatting
```python
‚úÖ DO:
gen_text = "Xin ch√†o! B·∫°n kh·ªèe kh√¥ng?"  # C√≥ d·∫•u c√¢u

‚ùå DON'T:
gen_text = "xin chao ban khoe khong"   # Thi·∫øu d·∫•u
```

### 3. Quality vs Speed
```python
# Development
nfe_step = 16, speed = 1.2  # Fast iteration

# Production
nfe_step = 32, speed = 1.0  # Balanced

# High Quality
nfe_step = 64, speed = 1.0  # Best quality
```

---

**Prev:** [`04-TRAINING-PIPELINE.md`](04-TRAINING-PIPELINE.md)  
**Next:** [`06-DATA-REQUIREMENTS.md`](06-DATA-REQUIREMENTS.md)



