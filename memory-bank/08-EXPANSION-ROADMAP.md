# 08 - Expansion Roadmap

> **Source:** T·ªïng h·ª£p t·ª´ YEUCAU.md - K·∫ø ho·∫°ch chi ti·∫øt h·ªá th·ªëng x·ª≠ l√Ω √¢m thanh v√† Clone gi·ªçng ti·∫øng Vi·ªát

## üéØ Vision

X√¢y d·ª±ng pipeline ho√†n ch·ªânh tr√™n Google Colab ƒë·ªÉ:
1. X·ª≠ l√Ω file podcast (lo·∫°i nh·∫°c n·ªÅn, t√°ch gi·ªçng)
2. Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán t·ª± ƒë·ªông
3. Training model clone gi·ªçng v·ªõi F5-TTS-Vietnamese
4. T·∫°o giao di·ªán s·ª≠ d·ª•ng ƒëa gi·ªçng

---

## üìã PHASE 1: Audio Preprocessing Pipeline

### 1.1 File Upload & Management

**Goal:** Upload nhi·ªÅu file MP3/WAV, qu·∫£n l√Ω t·∫≠p trung

**Features:**
```python
- Gradio FileUpload component (multi-file)
- Storage: /content/uploads/
- Metadata collection:
  * T√™n file g·ªëc
  * T√™n gi·ªçng (user input)
  * Duration, sample rate
```

**UI Components:**
```python
upload_area = gr.File(
    file_count="multiple",
    file_types=[".mp3", ".wav"],
    label="Upload Audio Files"
)

speaker_name = gr.Textbox(
    label="T√™n gi·ªçng",
    placeholder="Nh·∫≠p t√™n ng∆∞·ªùi n√≥i..."
)
```

### 1.2 Voice Separation (T√°ch gi·ªçng/nh·∫°c)

**Problem:** Podcast 30 ph√∫t c√≥ nh·∫°c n·ªÅn

**Solution: Demucs (RECOMMENDED)**

```yaml
Tool: Demucs (Facebook Research)
Model: htdemucs ho·∫∑c htdemucs_ft

Why Demucs:
  - SOTA trong voice separation
  - Pretrained t·ªët v·ªõi ti·∫øng Vi·ªát
  - X·ª≠ l√Ω nhanh tr√™n GPU
  - Quality cao

Process:
  Input: podcast.mp3 (30 ph√∫t)
  ‚Üì
  Demucs separation
  ‚Üì
  Output: vocals.wav (gi·ªçng n√≥i thu·∫ßn)
```

**Implementation:**
```python
import demucs.separate

def separate_vocals(audio_path, output_dir):
    """
    T√°ch gi·ªçng n√≥i kh·ªèi nh·∫°c n·ªÅn
    """
    # Demucs command
    cmd = [
        "python", "-m", "demucs.separate",
        "-n", "htdemucs",  # Model name
        "--two-stems", "vocals",  # Only vocals
        "-o", output_dir,
        audio_path
    ]
    
    subprocess.run(cmd)
    
    vocals_path = f"{output_dir}/htdemucs/{basename(audio_path)}/vocals.wav"
    return vocals_path
```

**Optimization cho file d√†i:**
```python
# Chunk processing
def process_long_audio(audio_path, chunk_duration=600):  # 10 ph√∫t/chunk
    """
    Chia file 30 ph√∫t th√†nh 3 chunks √ó 10 ph√∫t
    Process parallel n·∫øu c√≥ multi-GPU
    """
    chunks = split_audio(audio_path, chunk_duration)
    
    with concurrent.futures.ProcessPoolExecutor() as executor:
        results = executor.map(separate_vocals, chunks)
    
    # Merge results
    final_vocals = concatenate_audio(results)
    return final_vocals
```

**Alternative: Spleeter**
```python
# Backup n·∫øu Demucs qu√° ch·∫≠m
from spleeter.separator import Separator

separator = Separator('spleeter:2stems')  # vocals/accompaniment
separator.separate_to_file(audio_path, output_dir)
```

### 1.3 Voice Activity Detection (VAD)

**Goal:** Lo·∫°i b·ªè ƒëo·∫°n im l·∫∑ng, ch·ªâ gi·ªØ speech segments

**Solution: Silero VAD (RECOMMENDED)**

```yaml
Tool: Silero VAD
Why: T·ªët v·ªõi ti·∫øng Vi·ªát, fast, accurate

Process:
  1. Detect speech segments
  2. Lo·∫°i b·ªè silence > 0.5s
  3. Extract clean speech segments
  4. L∆∞u timestamps
```

**Implementation:**
```python
import torch
import torchaudio

# Load Silero VAD
model, utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad'
)

(get_speech_timestamps, _, _, _, _) = utils

def detect_speech(audio_path):
    """
    Detect speech segments
    """
    wav, sr = torchaudio.load(audio_path)
    
    # Get speech timestamps
    speech_timestamps = get_speech_timestamps(
        wav, 
        model,
        sampling_rate=sr,
        threshold=0.5,
        min_speech_duration_ms=500,
        min_silence_duration_ms=500
    )
    
    return speech_timestamps
```

### 1.4 Audio Quality Enhancement

**Goal:** C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng audio sau khi t√°ch

**Tools:**

1. **DeepFilterNet** - Noise reduction
```python
from deepfilternet import DeepFilterNet

model = DeepFilterNet()
clean_audio = model.enhance(noisy_audio)
```

2. **Resemble Enhance** - Audio super-resolution
```python
from resemble_enhance import enhance_audio

enhanced = enhance_audio(
    audio_path,
    output_sr=24000,
    denoise=True
)
```

**Processing Pipeline:**
```python
def enhance_audio(audio_path):
    """
    Complete enhancement pipeline
    """
    # 1. Load audio
    audio, sr = librosa.load(audio_path, sr=24000)
    
    # 2. Noise reduction
    audio = denoise(audio)
    
    # 3. Normalize volume
    audio = librosa.util.normalize(audio)
    
    # 4. Resample to 24kHz (F5-TTS requirement)
    audio = librosa.resample(audio, orig_sr=sr, target_sr=24000)
    
    return audio
```

---

## üìã PHASE 2: Automated Dataset Preparation

### 2.1 Audio Segmentation

**Goal:** Chia audio d√†i th√†nh clips ng·∫Øn 3-10s

**Smart Segmentation:**
```python
def smart_segment(audio_path, speech_timestamps):
    """
    Chia audio theo VAD timestamps + sentence boundaries
    """
    segments = []
    
    for ts in speech_timestamps:
        start, end = ts['start'], ts['end']
        duration = end - start
        
        # Filter by duration
        if duration < 2 or duration > 12:
            continue
        
        # Check SNR
        segment = extract_segment(audio_path, start, end)
        snr = calculate_snr(segment)
        if snr < 20:
            continue
        
        segments.append({
            'path': save_segment(segment),
            'start': start,
            'end': end,
            'duration': duration,
            'snr': snr
        })
    
    return segments
```

**Quality Filtering:**
```python
def filter_segments(segments):
    """
    Lo·∫°i b·ªè segments kh√¥ng ƒë·∫°t ch·∫•t l∆∞·ª£ng
    """
    filtered = []
    
    for seg in segments:
        # Duration check
        if seg['duration'] < 3 or seg['duration'] > 10:
            continue
        
        # SNR check
        if seg['snr'] < 20:
            continue
        
        # Music bleed-through check
        if detect_music_leak(seg['path']):
            continue
        
        filtered.append(seg)
    
    return filtered
```

### 2.2 Automatic Transcription

**Solution: Whisper Large-v3 (RECOMMENDED)**

```python
import whisper

model = whisper.load_model("large-v3")

def transcribe_audio(audio_path):
    """
    Transcribe ti·∫øng Vi·ªát v·ªõi Whisper
    """
    result = model.transcribe(
        audio_path,
        language="vi",  # Vietnamese
        task="transcribe",
        word_timestamps=True
    )
    
    return result['text']
```

**Batch Transcription:**
```python
def batch_transcribe(segments, batch_size=8):
    """
    Transcribe nhi·ªÅu segments c√πng l√∫c
    """
    transcriptions = []
    
    for i in tqdm(range(0, len(segments), batch_size)):
        batch = segments[i:i+batch_size]
        
        with concurrent.futures.ThreadPoolExecutor() as executor:
            results = executor.map(transcribe_audio, batch)
        
        transcriptions.extend(results)
    
    return transcriptions
```

**Alternative: FPT.AI ASR / VAIS ASR**
```python
# N·∫øu c·∫ßn accuracy cao h∆°n cho ti·∫øng Vi·ªát
# API-based, c·∫ßn internet

import requests

def fpt_transcribe(audio_path):
    """
    FPT.AI Speech-to-Text API
    """
    with open(audio_path, 'rb') as f:
        response = requests.post(
            'https://api.fpt.ai/hmi/asr/general',
            headers={'api-key': FPT_API_KEY},
            files={'file': f}
        )
    
    return response.json()['hypotheses'][0]['utterance']
```

### 2.3 Text Normalization

**Goal:** Chu·∫©n h√≥a text cho training

```python
def normalize_text(text):
    """
    Chu·∫©n h√≥a text ti·∫øng Vi·ªát
    """
    # 1. Lowercase
    text = text.lower()
    
    # 2. Remove special characters (gi·ªØ d·∫•u c√¢u quan tr·ªçng)
    text = re.sub(r'[^a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë\s,.\!\?]', '', text)
    
    # 3. Normalize numbers
    text = num2words(text, lang='vi')  # 123 ‚Üí m·ªôt trƒÉm hai m∆∞∆°i ba
    
    # 4. Handle abbreviations
    abbreviations = {
        'tp.': 'th√†nh ph·ªë',
        'ths.': 'th·∫°c sƒ©',
        # ... more
    }
    for abbr, full in abbreviations.items():
        text = text.replace(abbr, full)
    
    # 5. Unicode normalization (NFD)
    text = unicodedata.normalize('NFD', text)
    
    # 6. Clean whitespace
    text = ' '.join(text.split())
    
    return text
```

### 2.4 Dataset Organization

**Output Structure:**
```
/content/datasets/
‚îú‚îÄ‚îÄ speaker_001/
‚îÇ   ‚îú‚îÄ‚îÄ wavs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ segment_0001.wav
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ segment_0002.wav
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ metadata.csv
‚îÇ   ‚îî‚îÄ‚îÄ sample.wav  # Demo audio 5-10s
‚îú‚îÄ‚îÄ speaker_002/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ config.json
```

**metadata.csv Format:**
```csv
audio_path,text,speaker_name,duration,snr
wavs/segment_0001.wav,"xin ch√†o c√°c b·∫°n",speaker_001,3.2,28.5
wavs/segment_0002.wav,"h√¥m nay tr·ªùi ƒë·∫πp",speaker_001,4.1,31.2
```

**config.json:**
```json
{
  "speakers": [
    {
      "id": "speaker_001",
      "name": "Nguyen Van A",
      "total_duration": 1800.5,
      "num_segments": 350,
      "sample_audio": "sample.wav"
    }
  ]
}
```

---

## üìã PHASE 3: Multi-Speaker Training System

### 3.1 Training Configuration

**Hyperparameters cho podcast 30 ph√∫t:**
```yaml
Training:
  batch_size: 4000-7000 (t√πy GPU)
  learning_rate: 1e-5
  epochs: 50-100  # Kh√¥ng c·∫ßn qu√° nhi·ªÅu
  gradient_accumulation: 2
  mixed_precision: fp16

Data:
  sample_rate: 24000
  hop_length: 256
  max_audio_length: 10s

Early Stopping:
  monitor: validation_loss
  patience: 10
  
Checkpointing:
  save_every: 10 epochs
  keep_best: 3
```

### 3.2 Training Pipeline cho nhi·ªÅu gi·ªçng

**Workflow:**
```python
def train_new_speaker(audio_file, speaker_name):
    """
    Complete pipeline cho 1 gi·ªçng m·ªõi
    """
    # 1. Upload & save
    save_path = f"/content/uploads/{speaker_name}/"
    save_file(audio_file, save_path)
    
    # 2. Preprocessing
    vocals = separate_vocals(audio_file)
    segments = detect_and_segment(vocals)
    
    # 3. Transcription
    transcriptions = batch_transcribe(segments)
    
    # 4. Dataset preparation
    dataset_dir = prepare_dataset(segments, transcriptions, speaker_name)
    
    # 5. Training
    model = train_model(
        dataset_dir=dataset_dir,
        speaker_name=speaker_name,
        epochs=50
    )
    
    # 6. Save checkpoint
    save_checkpoint(model, f"ckpts/{speaker_name}/model_best.pt")
    
    return model
```

**Progress Tracking:**
```python
# UI components
progress_bar = gr.Progress()

def update_progress(stage, percentage):
    """
    Update training progress
    """
    stages = [
        "1. T√°ch gi·ªçng n√≥i...",
        "2. Ph√°t hi·ªán ƒëo·∫°n n√≥i...",
        "3. Transcription...",
        "4. Chu·∫©n b·ªã dataset...",
        "5. Training..."
    ]
    
    progress_bar(percentage, desc=stages[stage])
```

### 3.3 Checkpoint Management

**Structure:**
```
/content/models/
‚îú‚îÄ‚îÄ speaker_001/
‚îÇ   ‚îú‚îÄ‚îÄ best_model.pt
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ vocab.txt
‚îÇ   ‚îî‚îÄ‚îÄ sample_audio.wav
‚îú‚îÄ‚îÄ speaker_002/
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

**Google Drive Integration:**
```python
from google.colab import drive

# Mount Drive
drive.mount('/content/drive')

# Symlinks
models_dir = "/content/drive/MyDrive/voice_cloning/models"
!ln -s {models_dir} /content/models
```

---

## üìã PHASE 4: Production Interface

### 4.1 Gradio UI Layout

```python
with gr.Blocks() as app:
    gr.Markdown("# H·ªÜ TH·ªêNG CLONE GI·ªåNG TI·∫æNG VI·ªÜT")
    
    with gr.Tabs():
        # TAB 1: TRAINING
        with gr.Tab("Training"):
            with gr.Row():
                # Upload section
                upload_files = gr.File(
                    file_count="multiple",
                    label="Upload Audio (MP3/WAV)"
                )
                speaker_name = gr.Textbox(
                    label="T√™n gi·ªçng"
                )
            
            # Processing buttons
            with gr.Row():
                btn_separate = gr.Button("1. T√°ch gi·ªçng kh·ªèi nh·∫°c n·ªÅn")
                btn_prepare = gr.Button("2. Chu·∫©n b·ªã Dataset")
                btn_train = gr.Button("3. B·∫Øt ƒë·∫ßu Training")
            
            # Progress display
            progress_bar = gr.Progress()
            status_text = gr.Textbox(
                label="Status",
                lines=10,
                interactive=False
            )
            loss_plot = gr.Plot(label="Training Loss")
        
        # TAB 2: TEXT-TO-SPEECH
        with gr.Tab("Text-to-Speech"):
            # Speaker selection
            speaker_radio = gr.Radio(
                choices=list_available_speakers(),
                label="Ch·ªçn gi·ªçng"
            )
            
            # Demo audio
            demo_audio = gr.Audio(
                label="Demo gi·ªçng ƒë√£ ch·ªçn",
                autoplay=True
            )
            
            # Text input
            gen_text = gr.Textbox(
                label="Nh·∫≠p vƒÉn b·∫£n",
                lines=5,
                placeholder="Nh·∫≠p vƒÉn b·∫£n ti·∫øng Vi·ªát c·∫ßn chuy·ªÉn th√†nh gi·ªçng n√≥i..."
            )
            
            # Settings
            with gr.Accordion("Advanced Settings", open=False):
                speed_slider = gr.Slider(0.8, 1.5, 1.0, label="Speed")
                temperature = gr.Slider(0.1, 1.0, 0.7, label="Temperature")
                remove_silence = gr.Checkbox(label="Remove Silence")
            
            # Generate
            generate_btn = gr.Button("üéôÔ∏è T·∫°o gi·ªçng n√≥i", variant="primary")
            output_audio = gr.Audio(label="Audio Output")
            download_btn = gr.Button("üíæ L∆∞u audio")
```

### 4.2 Backend Functions

```python
def process_upload(audio_files, speaker_name):
    """
    Function 1: Process uploaded files
    """
    # Save files
    save_dir = f"/content/uploads/{speaker_name}/"
    os.makedirs(save_dir, exist_ok=True)
    
    for audio in audio_files:
        shutil.copy(audio, save_dir)
    
    # Run Demucs
    vocals = separate_vocals(audio_files[0])
    
    # VAD segmentation
    segments = segment_audio(vocals)
    
    # Whisper transcription
    transcriptions = transcribe_batch(segments)
    
    # Save to dataset folder
    dataset_dir = organize_dataset(
        segments, 
        transcriptions,
        speaker_name
    )
    
    return f"‚úÖ Processed {len(segments)} segments"

def train_speaker(speaker_name, epochs, batch_size):
    """
    Function 2: Train model
    """
    # Load dataset
    dataset_dir = f"/content/datasets/{speaker_name}"
    
    # Initialize model
    model = initialize_f5tts()
    
    # Training loop
    for epoch in range(epochs):
        loss = train_epoch(model, dataset_dir, batch_size)
        yield f"Epoch {epoch}/{epochs} | Loss: {loss:.4f}"
        
        # Save checkpoint
        if epoch % 10 == 0:
            save_checkpoint(model, f"model_epoch_{epoch}.pt")
    
    # Save final
    save_checkpoint(model, "model_best.pt")
    return "‚úÖ Training completed!"

def list_available_speakers():
    """
    Function 3: List trained speakers
    """
    models_dir = "/content/models/"
    speakers = [d for d in os.listdir(models_dir) if os.path.isdir(f"{models_dir}/{d}")]
    return speakers

def generate_speech(text, speaker_name, speed, temperature):
    """
    Function 4: Generate speech
    """
    # Load model
    model_path = f"/content/models/{speaker_name}/model_best.pt"
    model = load_model(model_path)
    
    # Text preprocessing
    text = normalize_text(text)
    
    # Inference
    audio = model.infer(
        gen_text=text,
        speed=speed,
        temperature=temperature
    )
    
    # Post-processing
    audio = postprocess(audio)
    
    return audio

def play_speaker_demo(speaker_name):
    """
    Function 5: Play demo audio
    """
    demo_path = f"/content/models/{speaker_name}/sample_audio.wav"
    return demo_path
```

---

## üìã PHASE 5-7: Optimization & Production

### Phase 5: Podcast Optimization

**Chunked Processing:**
```python
def process_long_podcast(audio_path, chunk_duration=300):
    """
    30 ph√∫t ‚Üí 6 chunks √ó 5 ph√∫t
    """
    chunks = split_audio(audio_path, chunk_duration)
    
    # Process parallel
    with concurrent.futures.ProcessPoolExecutor() as executor:
        results = list(executor.map(process_chunk, chunks))
    
    # Merge
    final_result = merge_results(results)
    return final_result
```

**Smart Caching:**
```python
import joblib

@joblib.Memory(location='/tmp/cache').cache
def separate_vocals_cached(audio_path):
    """
    Cache Demucs results
    """
    return separate_vocals(audio_path)
```

### Phase 6: Storage & Persistence

**Auto-save Strategy:**
```python
def auto_save_checkpoint(model, epoch, loss):
    """
    Save best model based on validation loss
    """
    if loss < best_loss:
        save_checkpoint(model, "model_best.pt")
        
        # Backup to Drive
        shutil.copy(
            "model_best.pt",
            "/content/drive/MyDrive/voice_cloning/backups/"
        )
```

### Phase 7: UX Enhancements

**Validation & Error Handling:**
```python
def validate_upload(audio_file):
    """
    Pre-processing checks
    """
    # Format check
    if not audio_file.endswith(('.wav', '.mp3')):
        raise ValueError("Ch·ªâ h·ªó tr·ª£ WAV v√† MP3")
    
    # Duration check
    duration = get_duration(audio_file)
    if duration < 60:
        raise ValueError("Audio qu√° ng·∫Øn (< 1 ph√∫t)")
    if duration > 3600:
        raise ValueError("Audio qu√° d√†i (> 1 gi·ªù). Vui l√≤ng chia nh·ªè.")
    
    # Quality check
    sr = get_sample_rate(audio_file)
    if sr < 16000:
        raise ValueError("Sample rate qu√° th·∫•p (< 16kHz)")
```

---

## üìä Timeline & Resource Estimation

### Time Estimates (per 30-min podcast):

```yaml
1. Upload: 1-2 ph√∫t
2. Voice Separation (Demucs): 5-10 ph√∫t (GPU)
3. VAD + Segmentation: 2-3 ph√∫t
4. Transcription (Whisper): 3-5 ph√∫t
5. Dataset Prep: 2 ph√∫t
6. Training (50 epochs): 2-4 gi·ªù

Total: ~3-4 gi·ªù t·ª´ upload ƒë·∫øn model ready
```

### Resource Requirements:

```yaml
RAM: 12-16GB (Colab Pro recommended)
GPU: T4 minimum, V100/A100 optimal
Disk: 5-10GB per speaker
Runtime: GPU runtime, High RAM
```

---

**Prev:** [`07-TECHNICAL-SPECS.md`](07-TECHNICAL-SPECS.md)  
**Next:** [`09-IMPLEMENTATION-GUIDE.md`](09-IMPLEMENTATION-GUIDE.md)



