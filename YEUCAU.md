# Káº¿ hoáº¡ch chi tiáº¿t: Há»‡ thá»‘ng xá»­ lÃ½ Ã¢m thanh vÃ  Clone giá»ng tiáº¿ng Viá»‡t

## ğŸ¯ Tá»•ng quan há»‡ thá»‘ng

XÃ¢y dá»±ng pipeline hoÃ n chá»‰nh trÃªn Google Colab Ä‘á»ƒ:
1. Xá»­ lÃ½ file podcast (loáº¡i nháº¡c ná»n, tÃ¡ch giá»ng)
2. Chuáº©n bá»‹ dá»¯ liá»‡u huáº¥n luyá»‡n
3. Training model clone giá»ng vá»›i F5-TTS-Vietnamese
4. Táº¡o giao diá»‡n sá»­ dá»¥ng Ä‘a giá»ng

---

## ğŸ“‹ PHASE 1: Tiá»n xá»­ lÃ½ Audio (Audio Preprocessing)

### 1.1. Upload vÃ  quáº£n lÃ½ file
**Má»¥c tiÃªu**: Cho phÃ©p ngÆ°á»i dÃ¹ng upload nhiá»u file MP3/WAV

**CÃ´ng nghá»‡**:
- Gradio FileUpload component
- LÆ°u trá»¯ táº¡m trong `/content/uploads/`
- Há»— trá»£ batch upload nhiá»u file cÃ¹ng lÃºc

**ThÃ´ng tin cáº§n thu tháº­p**:
- TÃªn file gá»‘c
- TÃªn giá»ng (do user Ä‘áº·t) - text input
- Metadata: duration, sample rate

### 1.2. TÃ¡ch giá»ng nÃ³i khá»i nháº¡c ná»n (Voice Separation)
**Váº¥n Ä‘á»**: File podcast 30 phÃºt cÃ³ nháº¡c ná»n

**Giáº£i phÃ¡p Ä‘á» xuáº¥t**:

#### Option 1: **Demucs** (Facebook Research) - Äá»€ XUáº¤T
- **LÃ½ do chá»n**: 
  - SOTA trong voice separation
  - Pretrained model tá»‘t vá»›i tiáº¿ng Viá»‡t
  - Xá»­ lÃ½ nhanh trÃªn GPU
  - Model: `htdemucs` hoáº·c `htdemucs_ft`

- **Quy trÃ¬nh**:
  ```
  Input MP3 (30 phÃºt) 
  â†’ Demucs separation 
  â†’ Output: vocals.wav (giá»ng nÃ³i thuáº§n)
  ```

#### Option 2: **Spleeter** (Deezer)
- Backup option náº¿u Demucs quÃ¡ cháº­m
- Model 2stems (vocals/accompaniment)

**Tá»‘i Æ°u xá»­ lÃ½ file 30 phÃºt**:
- Chunk processing: chia file thÃ nh segments 5-10 phÃºt
- Process parallel náº¿u cÃ³ multi-GPU
- Ãp dá»¥ng batch processing cho nhiá»u file

### 1.3. Voice Activity Detection (VAD)
**Má»¥c tiÃªu**: Loáº¡i bá» Ä‘oáº¡n im láº·ng, chá»‰ giá»¯ láº¡i speech segments

**CÃ´ng nghá»‡**:
- **Silero VAD** (Ä‘á» xuáº¥t) - tá»‘t vá»›i tiáº¿ng Viá»‡t
- Hoáº·c **WebRTC VAD**

**Quy trÃ¬nh**:
1. Detect speech segments
2. Loáº¡i bá» silence > 0.5s
3. Extract clean speech segments
4. LÆ°u timestamps cho má»—i segment

### 1.4. Audio Quality Enhancement (Optional nhÆ°ng quan trá»ng)
**Má»¥c tiÃªu**: Cáº£i thiá»‡n cháº¥t lÆ°á»£ng audio sau khi tÃ¡ch

**CÃ´ng nghá»‡**:
- **DeepFilterNet**: Noise reduction
- **Resemble Enhance**: Audio super-resolution

**Ãp dá»¥ng**:
- Noise reduction
- Normalize volume
- Resample vá» 24kHz (yÃªu cáº§u cá»§a F5-TTS)

---

## ğŸ“‹ PHASE 2: Chuáº©n bá»‹ Dataset cho Training

### 2.1. Audio Segmentation
**Má»¥c tiÃªu**: Chia audio dÃ i thÃ nh clips ngáº¯n phÃ¹ há»£p training

**YÃªu cáº§u F5-TTS**:
- Duration: 3-10 giÃ¢y/clip (optimal: 5-7s)
- Format: WAV, 24kHz, mono
- Cháº¥t lÆ°á»£ng: SNR > 20dB

**Chiáº¿n lÆ°á»£c chia segments**:
1. **Smart Segmentation**:
   - DÃ¹ng VAD timestamps
   - Chia theo cÃ¢u hoÃ n chá»‰nh (dÃ¹ng pause detection)
   - TrÃ¡nh cáº¯t giá»¯a tá»«

2. **Filtering**:
   - Loáº¡i clip < 2s hoáº·c > 12s
   - Loáº¡i clip cÃ³ SNR tháº¥p
   - Loáº¡i clip cÃ³ music bleed-through cÃ²n sÃ³t

3. **Quality Check**:
   - Auto-detect clips cÃ³ váº¥n Ä‘á»
   - Manual review interface (nghe máº«u ngáº«u nhiÃªn)

### 2.2. Transcription (Chuyá»ƒn Ã¢m thanh thÃ nh text)
**Váº¥n Ä‘á»**: F5-TTS cáº§n cáº·p (audio, text) Ä‘á»ƒ training

**Giáº£i phÃ¡p**:

#### Option 1: **Whisper Large-v3** - Äá»€ XUáº¤T
- Accuracy cao nháº¥t vá»›i tiáº¿ng Viá»‡t
- Model: `openai/whisper-large-v3`
- CÃ³ timestamp alignment

#### Option 2: **FPT.AI ASR** hoáº·c **VAIS ASR**
- Náº¿u cáº§n accuracy cao hÆ¡n cho tiáº¿ng Viá»‡t
- API-based (cáº§n internet)

**Quy trÃ¬nh**:
1. Transcribe tá»«ng segment
2. LÆ°u text file vá»›i cÃ¹ng tÃªn audio
3. Format: `segment_001.wav` â†’ `segment_001.txt`

**Tá»‘i Æ°u cho file 30 phÃºt**:
- Batch transcription
- Cache results
- Progress bar hiá»ƒn thá»‹

### 2.3. Text Normalization
**Má»¥c tiÃªu**: Chuáº©n hÃ³a text cho training

**Xá»­ lÃ½**:
- Lowercase (náº¿u cáº§n)
- Remove special characters khÃ´ng cáº§n thiáº¿t
- Chuáº©n hÃ³a sá»‘ â†’ chá»¯ (123 â†’ má»™t trÄƒm hai mÆ°Æ¡i ba)
- Xá»­ lÃ½ viáº¿t táº¯t
- Äáº£m báº£o Unicode NFD normalization

### 2.4. Dataset Organization
**Cáº¥u trÃºc thÆ° má»¥c**:
```
/content/datasets/
â”œâ”€â”€ speaker_001/
â”‚   â”œâ”€â”€ wavs/
â”‚   â”‚   â”œâ”€â”€ segment_001.wav
â”‚   â”‚   â”œâ”€â”€ segment_002.wav
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ metadata.csv  # path|text|speaker_id
â”‚   â””â”€â”€ sample.wav    # Audio demo 5-10s
â”œâ”€â”€ speaker_002/
â”‚   â””â”€â”€ ...
â””â”€â”€ config.json       # LÆ°u thÃ´ng tin speakers
```

**Metadata Format**:
```csv
audio_path,text,speaker_name,duration
wavs/segment_001.wav,"xin chÃ o cÃ¡c báº¡n",speaker_001,3.2
```

---

## ğŸ“‹ PHASE 3: Training vá»›i F5-TTS-Vietnamese

### 3.1. Setup Environment
**CÃ i Ä‘áº·t**:
1. Install dependencies
2. Download pretrained base model (náº¿u cÃ³)
3. Setup GPU (T4/V100 trÃªn Colab)

### 3.2. Training Configuration
**Hyperparameters cáº§n Ä‘iá»u chá»‰nh**:

```yaml
# Training config
batch_size: 4-8 (tÃ¹y GPU memory)
learning_rate: 1e-4
max_epochs: 50-100
gradient_accumulation: 2
mixed_precision: fp16

# Data config
sample_rate: 24000
hop_length: 256
max_audio_length: 10s

# Speaker embedding
speaker_embedding_dim: 256
```

**Chiáº¿n lÆ°á»£c Training**:
1. **Quick Training** (cho podcast 30 phÃºt):
   - Epochs: 50-100 (khÃ´ng cáº§n quÃ¡ nhiá»u)
   - Early stopping: monitor validation loss
   - Checkpoint má»—i 10 epochs

2. **Multi-speaker Training**:
   - Train riÃªng cho tá»«ng speaker â†’ cÃ¡c checkpoints Ä‘á»™c láº­p
   - HOáº¶C multi-speaker model vá»›i speaker embeddings

### 3.3. Training Pipeline cho nhiá»u giá»ng
**Workflow**:
1. User upload file MP3 má»›i
2. Click "ThÃªm giá»ng má»›i" â†’ nháº­p tÃªn
3. Tá»± Ä‘á»™ng xá»­ lÃ½ pipeline PHASE 1 + 2
4. Click "Báº¯t Ä‘áº§u Training"
5. Progress bar hiá»ƒn thá»‹:
   - Data preprocessing: X%
   - Training: Epoch Y/Z, Loss: W
   - ETA: M phÃºt

**Quáº£n lÃ½ checkpoints**:
```
/content/models/
â”œâ”€â”€ speaker_001/
â”‚   â”œâ”€â”€ best_model.pth
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ sample_audio.wav
â”œâ”€â”€ speaker_002/
â”‚   â””â”€â”€ ...
```

### 3.4. Tá»‘i Æ°u Training Speed
**Cho file podcast 30 phÃºt**:
- Expected segments: 200-300 clips (5-7s/clip)
- Training time Æ°á»›c tÃ­nh: 2-4 giá» trÃªn T4 GPU
- Tricks:
  - Mixed precision training (fp16)
  - Gradient checkpointing
  - DataLoader num_workers=2
  - Batch size optimal

---

## ğŸ“‹ PHASE 4: Inference Interface (Giao diá»‡n sá»­ dá»¥ng)

### 4.1. Gradio UI Components

#### Layout tá»•ng thá»ƒ:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Há»† THá»NG CLONE GIá»ŒNG TIáº¾NG VIá»†T        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [TAB 1: TRAINING]                      â”‚
â”‚  - Upload Audio                         â”‚
â”‚  - Xá»­ lÃ½ & Training                     â”‚
â”‚                                         â”‚
â”‚  [TAB 2: TEXT-TO-SPEECH]               â”‚
â”‚  - Chá»n giá»ng                          â”‚
â”‚  - Nháº­p text                           â”‚
â”‚  - Generate                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### TAB 1: Training Interface
**Components**:
1. **File Upload Area**:
   - `gr.File(file_count="multiple")` - upload nhiá»u file
   - Accept: .mp3, .wav
   - Display: danh sÃ¡ch files Ä‘Ã£ upload

2. **Speaker Management**:
   - `gr.Textbox()` - Nháº­p tÃªn giá»ng
   - `gr.Button("ThÃªm giá»ng má»›i")`
   - `gr.Dropdown()` - Chá»n giá»ng Ä‘ang xá»­ lÃ½

3. **Processing Pipeline**:
   - `gr.Button("1. TÃ¡ch giá»ng khá»i nháº¡c ná»n")`
   - `gr.Button("2. Chuáº©n bá»‹ Dataset")`
   - `gr.Button("3. Báº¯t Ä‘áº§u Training")`
   - Progress bars cho má»—i bÆ°á»›c

4. **Status Display**:
   - `gr.Textbox()` - Hiá»ƒn thá»‹ logs
   - `gr.Plot()` - Training curves (loss)

#### TAB 2: Text-to-Speech Interface
**Components**:
1. **Speaker Selection**:
   - `gr.Radio()` - Chá»n giá»ng
   - Auto-load available speakers tá»« `/content/models/`
   - Khi click â†’ auto play sample audio

2. **Demo Audio Player**:
   - `gr.Audio()` - PhÃ¡t sample cá»§a giá»ng Ä‘Æ°á»£c chá»n
   - Auto-trigger khi Ä‘á»•i giá»ng

3. **Text Input**:
   - `gr.Textbox(lines=5)` - Nháº­p text tiáº¿ng Viá»‡t
   - Placeholder: "Nháº­p vÄƒn báº£n tiáº¿ng Viá»‡t cáº§n chuyá»ƒn thÃ nh giá»ng nÃ³i..."
   - Character counter: hiá»ƒn thá»‹ Ä‘á»™ dÃ i

4. **Generation Settings**:
   - `gr.Slider()` - Speed (0.8 - 1.5x)
   - `gr.Slider()` - Temperature (creativity)
   - `gr.Checkbox()` - Enable/disable post-processing

5. **Generate Button**:
   - `gr.Button("ğŸ™ï¸ Táº¡o giá»ng nÃ³i")`
   - Processing indicator

6. **Output**:
   - `gr.Audio()` - PhÃ¡t vÃ  download audio sinh ra
   - `gr.Button("ğŸ’¾ LÆ°u audio")`

### 4.2. Backend Functions

#### Function 1: `process_upload(audio_files, speaker_name)`
**Input**: List audio files, tÃªn giá»ng
**Output**: Processed data ready for training
**Steps**:
1. Save files to `/content/uploads/{speaker_name}/`
2. Run Demucs separation
3. VAD segmentation
4. Whisper transcription
5. Save to dataset folder

#### Function 2: `train_speaker(speaker_name, epochs, batch_size)`
**Input**: Config training
**Output**: Trained model checkpoint
**Steps**:
1. Load dataset
2. Initialize F5-TTS model
3. Training loop vá»›i progress updates
4. Save best checkpoint

#### Function 3: `list_available_speakers()`
**Output**: List speakers Ä‘Ã£ train
**Logic**: Scan `/content/models/` folder

#### Function 4: `generate_speech(text, speaker_name, speed, temperature)`
**Input**: Text + config
**Output**: Audio file
**Steps**:
1. Load model checkpoint
2. Text preprocessing
3. F5-TTS inference
4. Post-processing
5. Return audio

#### Function 5: `play_speaker_demo(speaker_name)`
**Input**: TÃªn giá»ng
**Output**: Sample audio
**Logic**: Load `sample_audio.wav` tá»« model folder

---

## ğŸ“‹ PHASE 5: Tá»‘i Æ°u cho Podcast 30 phÃºt

### 5.1. Processing Pipeline Optimization

**Strategy 1: Chunked Processing**
```
30 phÃºt podcast
â†“
Chia thÃ nh 6 chunks Ã— 5 phÃºt
â†“
Process parallel (náº¿u cÃ³ multi-CPU)
â†“
Merge results
```

**Strategy 2: Smart Caching**
- Cache káº¿t quáº£ Demucs separation
- Cache transcription results
- Reuse náº¿u process láº¡i

**Strategy 3: Progressive Processing**
- Hiá»ƒn thá»‹ progress real-time
- Cho phÃ©p dá»«ng/tiáº¿p tá»¥c
- Save intermediate results

### 5.2. Memory Management
**Váº¥n Ä‘á»**: File 30 phÃºt â†’ ~90MB RAM

**Giáº£i phÃ¡p**:
- Stream processing thay vÃ¬ load toÃ n bá»™
- Clear cache sau má»—i bÆ°á»›c
- Garbage collection
- Monitor GPU memory

### 5.3. Quality vs Speed Tradeoff
**Fast Mode** (10-15 phÃºt processing):
- Demucs with lower quality setting
- Skip enhancement
- Basic VAD

**High Quality Mode** (30-45 phÃºt processing):
- Best Demucs model
- DeepFilterNet enhancement
- Careful segmentation
- Manual review option

---

## ğŸ“‹ PHASE 6: Storage & Persistence

### 6.1. Google Drive Integration
**Má»¥c tiÃªu**: LÆ°u models, datasets lÃ¢u dÃ i

**Setup**:
```python
from google.colab import drive
drive.mount('/content/drive')

# Symlinks
/content/models â†’ /content/drive/MyDrive/voice_cloning/models
/content/datasets â†’ /content/drive/MyDrive/voice_cloning/datasets
```

### 6.2. Auto-save Strategy
- Auto-save checkpoints má»—i N epochs
- Save best model based on validation loss
- Backup config files

### 6.3. Export/Import Speakers
**Features**:
- Export speaker package (model + config + sample)
- Import speaker tá»« .zip file
- Share speakers giá»¯a sessions

---

## ğŸš€ PHASE 7: User Experience Enhancements

### 7.1. Validation & Error Handling
**Pre-processing checks**:
- File format validation
- Audio quality check (sample rate, channels)
- Duration limits
- Warning náº¿u cÃ³ nhiá»u background noise

**Training checks**:
- Minimum data requirements (Ã­t nháº¥t 100 segments)
- GPU availability
- Disk space

### 7.2. Helpful Features
1. **Tutorial Mode**: 
   - Guided walkthrough cho láº§n Ä‘áº§u
   - Example files Ä‘á»ƒ test

2. **Quality Metrics**:
   - Hiá»ƒn thá»‹ data quality score
   - SNR cá»§a tá»«ng segment
   - Transcription confidence

3. **Comparison Tool**:
   - So sÃ¡nh giá»ng gá»‘c vs generated
   - A/B testing interface

4. **Batch Generation**:
   - Input multiple texts
   - Generate all vá»›i cÃ¹ng giá»ng
   - Download as ZIP

---

## ğŸ“Š Timeline & Resource Estimation

### Time Estimates (per 30-min podcast):
1. **Upload**: 1-2 phÃºt (tÃ¹y bandwidth)
2. **Voice Separation**: 5-10 phÃºt (GPU)
3. **VAD + Segmentation**: 2-3 phÃºt
4. **Transcription**: 3-5 phÃºt (Whisper)
5. **Dataset Prep**: 2 phÃºt
6. **Training**: 2-4 giá» (50-100 epochs)

**Total**: ~3-4 giá» tá»« upload Ä‘áº¿n cÃ³ model sá»­ dá»¥ng Ä‘Æ°á»£c

### Resource Requirements:
- **RAM**: 12-16GB (Colab Pro recommended)
- **GPU**: T4 minimum, V100/A100 optimal
- **Disk**: 5-10GB per speaker (raw + processed)
- **Runtime**: GPU runtime, High RAM

---

## ğŸ¯ Phá»¥ lá»¥c: Technical Stack Summary

### Core Libraries:
```
# Audio Processing
- demucs (separation)
- silero-vad (voice detection)
- librosa (audio manipulation)
- pydub (format conversion)

# Speech Recognition
- openai-whisper (transcription)

# Voice Cloning
- F5-TTS-Vietnamese (main model)

# UI
- gradio (interface)

# Utilities
- torch, torchaudio
- numpy, scipy
- pandas (metadata)
```

### Installation Priority:
1. Core audio libs (demucs, whisper)
2. F5-TTS repo + dependencies
3. Gradio UI
4. Enhancement tools (optional)

Káº¿ hoáº¡ch nÃ y Ä‘áº£m báº£o:
âœ… Xá»­ lÃ½ tá»‘t podcast 30 phÃºt cÃ³ nháº¡c ná»n  
âœ… Training nhanh vÃ  hiá»‡u quáº£  
âœ… Multi-speaker support  
âœ… UI/UX thÃ¢n thiá»‡n  
âœ… Tá»‘i Æ°u cho Google Colab  
âœ… TÆ°Æ¡ng thÃ­ch tá»‘t vá»›i tiáº¿ng Viá»‡t